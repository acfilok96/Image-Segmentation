{
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LTYlk-tvJOCk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "544a2369-0ba8-4e60-b892-320c13b08290",
        "execution": {
          "iopub.status.busy": "2023-05-03T18:37:16.077438Z",
          "iopub.execute_input": "2023-05-03T18:37:16.077779Z",
          "iopub.status.idle": "2023-05-03T18:37:16.081451Z",
          "shell.execute_reply.started": "2023-05-03T18:37:16.077752Z",
          "shell.execute_reply": "2023-05-03T18:37:16.080520Z"
        },
        "trusted": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "id": "LTYlk-tvJOCk"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir())"
      ],
      "metadata": {
        "id": "e30015bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28154b79-f9c7-47f4-ac61-fec75bc7f502",
        "execution": {
          "iopub.status.busy": "2023-05-03T18:37:16.087739Z",
          "iopub.execute_input": "2023-05-03T18:37:16.088479Z",
          "iopub.status.idle": "2023-05-03T18:37:16.096206Z",
          "shell.execute_reply.started": "2023-05-03T18:37:16.088446Z",
          "shell.execute_reply": "2023-05-03T18:37:16.095261Z"
        },
        "trusted": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'drive', 'sample_data']\n"
          ]
        }
      ],
      "id": "e30015bb"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os,sys\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "random.seed(1)\n",
        "np.random.seed(1)\n",
        "tf.random.set_seed(2)\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(10)\n",
        "\n",
        "import os, keras, numpy,tensorflow\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import *\n",
        "from numpy.random import *\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras.layers import *\n",
        "from tensorflow.keras import initializers\n",
        "\n",
        "from keras.layers import *\n",
        "from keras.layers import Dense\n",
        "from keras.layers.core import Activation\n",
        "from keras.initializers import RandomNormal\n",
        "from keras.layers.convolutional import UpSampling2D\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers import Input\n",
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose, UpSampling2D\n",
        "from keras.models import Model\n",
        "from keras.layers import add\n",
        "from keras.layers import Conv2D, Conv2DTranspose, BatchNormalization"
      ],
      "metadata": {
        "id": "832bae2f",
        "execution": {
          "iopub.status.busy": "2023-05-03T18:37:16.098545Z",
          "iopub.execute_input": "2023-05-03T18:37:16.099282Z",
          "iopub.status.idle": "2023-05-03T18:37:16.112104Z",
          "shell.execute_reply.started": "2023-05-03T18:37:16.099248Z",
          "shell.execute_reply": "2023-05-03T18:37:16.111153Z"
        },
        "trusted": true
      },
      "execution_count": 3,
      "outputs": [],
      "id": "832bae2f"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "with tf.device(device_name):\n",
        "    print(device_name.split(\":\")[1],\" running . . . \")"
      ],
      "metadata": {
        "id": "a8745138",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6036270-0827-4c2e-c328-1a91f0d3e326",
        "execution": {
          "iopub.status.busy": "2023-05-03T18:37:16.113869Z",
          "iopub.execute_input": "2023-05-03T18:37:16.114271Z",
          "iopub.status.idle": "2023-05-03T18:37:16.129364Z",
          "shell.execute_reply.started": "2023-05-03T18:37:16.114241Z",
          "shell.execute_reply": "2023-05-03T18:37:16.128426Z"
        },
        "trusted": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU  running . . . \n"
          ]
        }
      ],
      "id": "a8745138"
    },
    {
      "cell_type": "code",
      "source": [
        "# strategy = tf.distribute.MirroredStrategy()\n",
        "# print(\"Number of Accelerator: \",strategy.num_replicas_in_sync)\n",
        "\n",
        "# # batch_size = 256\n",
        "# # epoch = 1\n",
        "\n",
        "# # BATCH_SIZE =  batch_size*strategy.num_replicas_in_sync\n",
        "# # print(\"Global Batch Size: \",BATCH_SIZE)\n",
        "\n",
        "# with strategy.scope():\n",
        "#   # model = create_model()\n",
        "#   print(\"Ready To Go !\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-03T18:37:16.133217Z",
          "iopub.execute_input": "2023-05-03T18:37:16.133494Z",
          "iopub.status.idle": "2023-05-03T18:37:16.150131Z",
          "shell.execute_reply.started": "2023-05-03T18:37:16.133469Z",
          "shell.execute_reply": "2023-05-03T18:37:16.149134Z"
        },
        "trusted": true,
        "id": "CD2uTilm5_7x"
      },
      "execution_count": 5,
      "outputs": [],
      "id": "CD2uTilm5_7x"
    },
    {
      "cell_type": "code",
      "source": [
        "def Discriminator(input_shape_in = (128, 128, 3), target_shape_in = (128, 128, 3)):\n",
        "    \n",
        "    # input\n",
        "    input_shape = Input(shape = input_shape_in)\n",
        "    input_target = Input(shape = target_shape_in)\n",
        "    \n",
        "    concate_one = Concatenate()([input_shape, input_target])\n",
        "    \n",
        "    # C64\n",
        "    model = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=RandomNormal(stddev=0.02))(concate_one)\n",
        "    model = LeakyReLU(alpha=0.2)(model)\n",
        "    \n",
        "    # C128\n",
        "    model = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=RandomNormal(stddev=0.02))(model)\n",
        "    model = BatchNormalization()(model)\n",
        "    model = LeakyReLU(alpha=0.2)(model)\n",
        "    \n",
        "    # C256\n",
        "    model = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=RandomNormal(stddev=0.02))(model)\n",
        "    model = BatchNormalization()(model)\n",
        "    model = LeakyReLU(alpha=0.2)(model)\n",
        "    \n",
        "    # C512\n",
        "    model = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=RandomNormal(stddev=0.02))(model)\n",
        "    model = BatchNormalization()(model)\n",
        "    model = LeakyReLU(alpha=0.2)(model)\n",
        "\n",
        "    # C512\n",
        "    model = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=RandomNormal(stddev=0.02))(model)\n",
        "    model = BatchNormalization()(model)\n",
        "    model = LeakyReLU(alpha=0.2)(model)\n",
        "    \n",
        "    # second last output layer\n",
        "    model = Conv2D(512, (4,4), padding='same', kernel_initializer=RandomNormal(stddev=0.02))(model)\n",
        "    model = BatchNormalization()(model)\n",
        "    model = LeakyReLU(alpha=0.2)(model)\n",
        "    \n",
        "    # patch output\n",
        "    model = Conv2D(1, (4,4), padding='same', kernel_initializer=RandomNormal(stddev=0.02))(model)\n",
        "    model = Activation('tanh')(model)\n",
        "    \n",
        "    # define model\n",
        "    discriminator_model = Model(inputs = [input_shape, input_target], outputs = model)\n",
        "    \n",
        "    # compile model\n",
        "    opt = Adam(learning_rate=0.0004, beta_1=0.5)\n",
        "    discriminator_model.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[0.5])\n",
        "        \n",
        "    return discriminator_model"
      ],
      "metadata": {
        "id": "bde13e23",
        "execution": {
          "iopub.status.busy": "2023-05-03T18:37:16.151637Z",
          "iopub.execute_input": "2023-05-03T18:37:16.152225Z",
          "iopub.status.idle": "2023-05-03T18:37:16.162524Z",
          "shell.execute_reply.started": "2023-05-03T18:37:16.152195Z",
          "shell.execute_reply": "2023-05-03T18:37:16.161761Z"
        },
        "trusted": true
      },
      "execution_count": 6,
      "outputs": [],
      "id": "bde13e23"
    },
    {
      "cell_type": "code",
      "source": [
        "a = Discriminator()\n",
        "print(a.summary())"
      ],
      "metadata": {
        "id": "470acffc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25d84b99-6164-4b7c-b2f3-aea8881bbd7d",
        "execution": {
          "iopub.status.busy": "2023-05-03T18:37:16.164044Z",
          "iopub.execute_input": "2023-05-03T18:37:16.164606Z",
          "iopub.status.idle": "2023-05-03T18:37:16.433597Z",
          "shell.execute_reply.started": "2023-05-03T18:37:16.164532Z",
          "shell.execute_reply": "2023-05-03T18:37:16.432737Z"
        },
        "trusted": true
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 128, 128, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 128, 128, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 128, 128, 6)  0           ['input_1[0][0]',                \n",
            "                                                                  'input_2[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 64, 64, 64)   6208        ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " leaky_re_lu (LeakyReLU)        (None, 64, 64, 64)   0           ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 32, 32, 128)  131200      ['leaky_re_lu[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 32, 32, 128)  512        ['conv2d_1[0][0]']               \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " leaky_re_lu_1 (LeakyReLU)      (None, 32, 32, 128)  0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 16, 16, 256)  524544      ['leaky_re_lu_1[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 16, 16, 256)  1024       ['conv2d_2[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " leaky_re_lu_2 (LeakyReLU)      (None, 16, 16, 256)  0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 8, 8, 256)    1048832     ['leaky_re_lu_2[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 8, 8, 256)   1024        ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " leaky_re_lu_3 (LeakyReLU)      (None, 8, 8, 256)    0           ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 4, 4, 512)    2097664     ['leaky_re_lu_3[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 4, 4, 512)   2048        ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " leaky_re_lu_4 (LeakyReLU)      (None, 4, 4, 512)    0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 4, 4, 512)    4194816     ['leaky_re_lu_4[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 4, 4, 512)   2048        ['conv2d_5[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " leaky_re_lu_5 (LeakyReLU)      (None, 4, 4, 512)    0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 4, 4, 1)      8193        ['leaky_re_lu_5[0][0]']          \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 4, 4, 1)      0           ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 8,018,113\n",
            "Trainable params: 8,014,785\n",
            "Non-trainable params: 3,328\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "id": "470acffc"
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the discriminator model\n",
        "# tf.keras.utils.plot_model(a,show_shapes=True,\n",
        "#                           show_dtype=True,\n",
        "#                           show_layer_names=True,\n",
        "#                           show_layer_activations=True)"
      ],
      "metadata": {
        "id": "fcd2769a",
        "execution": {
          "iopub.status.busy": "2023-05-03T18:37:16.434658Z",
          "iopub.execute_input": "2023-05-03T18:37:16.434967Z",
          "iopub.status.idle": "2023-05-03T18:37:16.927908Z",
          "shell.execute_reply.started": "2023-05-03T18:37:16.434937Z",
          "shell.execute_reply": "2023-05-03T18:37:16.927063Z"
        },
        "trusted": true
      },
      "execution_count": 8,
      "outputs": [],
      "id": "fcd2769a"
    },
    {
      "cell_type": "code",
      "source": [
        "# define an encoder block\n",
        "def define_encoder_block(layer_in, n_filters, batchnorm=True):\n",
        "    g = Conv2D(n_filters, (3,3), strides=(2,2), padding='same', kernel_initializer=RandomNormal(stddev=0.02))(layer_in)\n",
        "    if batchnorm:\n",
        "        g = BatchNormalization()(g, training=True)\n",
        "    g = LeakyReLU(alpha=0.2)(g)\n",
        "    \n",
        "    return g\n"
      ],
      "metadata": {
        "id": "edef3b46",
        "execution": {
          "iopub.status.busy": "2023-05-03T18:37:16.930808Z",
          "iopub.execute_input": "2023-05-03T18:37:16.931130Z",
          "iopub.status.idle": "2023-05-03T18:37:16.937024Z",
          "shell.execute_reply.started": "2023-05-03T18:37:16.931103Z",
          "shell.execute_reply": "2023-05-03T18:37:16.936062Z"
        },
        "trusted": true
      },
      "execution_count": 9,
      "outputs": [],
      "id": "edef3b46"
    },
    {
      "cell_type": "code",
      "source": [
        "# define a decoder block\n",
        "def decoder_block(layer_in, skip_in, n_filters, dropout=True):\n",
        "    # g = Conv2DTranspose(n_filters, (3,3), strides=(2,2), padding='same', kernel_initializer=RandomNormal(stddev=0.02))(layer_in)\n",
        "    g = UpSampling2D(size=(2,2))(layer_in)\n",
        "    g = Conv2D(n_filters, (1,1), padding='same', kernel_initializer=RandomNormal(stddev=0.02))(g)\n",
        "    g = BatchNormalization()(g, training=True)\n",
        "    if dropout:\n",
        "        g = Dropout(0.5)(g, training=True)\n",
        "    g = Concatenate()([g, skip_in])\n",
        "    g = Activation('relu')(g)\n",
        "    \n",
        "    return g"
      ],
      "metadata": {
        "id": "494fafc5",
        "execution": {
          "iopub.status.busy": "2023-05-03T18:37:16.938810Z",
          "iopub.execute_input": "2023-05-03T18:37:16.939549Z",
          "iopub.status.idle": "2023-05-03T18:37:16.951550Z",
          "shell.execute_reply.started": "2023-05-03T18:37:16.939516Z",
          "shell.execute_reply": "2023-05-03T18:37:16.950837Z"
        },
        "trusted": true
      },
      "execution_count": 10,
      "outputs": [],
      "id": "494fafc5"
    },
    {
      "cell_type": "code",
      "source": [
        "# define the standalone generator model\n",
        "def Generator(image_shape=(128,128,3)):\n",
        "   \n",
        "    # image input\n",
        "    in_image = Input(shape=image_shape)\n",
        "    \n",
        "    # encoder model\n",
        "    e1 = define_encoder_block(in_image, 64, batchnorm=False)\n",
        "    e2 = define_encoder_block(e1, 128)\n",
        "    e3 = define_encoder_block(e2, 256)\n",
        "    e4 = define_encoder_block(e3, 512)\n",
        "    e5 = define_encoder_block(e4, 512)\n",
        "    e6 = define_encoder_block(e5, 512)\n",
        "    \n",
        "    # bottleneck, no batch norm and relu\n",
        "    b = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=RandomNormal(stddev=0.02))(e6)\n",
        "    b = Activation('relu')(b)\n",
        "    \n",
        "    # decoder model\n",
        "    d2 = decoder_block(b, e6, 512)\n",
        "    d3 = decoder_block(d2, e5, 512)\n",
        "    d4 = decoder_block(d3, e4, 512, dropout=False)\n",
        "    d5 = decoder_block(d4, e3, 256, dropout=False)\n",
        "    d6 = decoder_block(d5, e2, 128, dropout=False)\n",
        "    d7 = decoder_block(d6, e1, 64, dropout=False)\n",
        "    \n",
        "    # output\n",
        "    # g = Conv2DTranspose(3, (3,3), strides=(2,2), padding='same', kernel_initializer=RandomNormal(stddev=0.02))(d7)\n",
        "    g = UpSampling2D(size=(2,2))(d7)\n",
        "    g = Conv2D(64, (1,1), padding='same', kernel_initializer=RandomNormal(stddev=0.02))(g)\n",
        "\n",
        "    g = Conv2D(3, (4,4), padding='same', kernel_initializer=RandomNormal(stddev=0.02))(g)\n",
        "    out_image = Activation('tanh')(g)\n",
        "    \n",
        "    # model\n",
        "    model = Model(in_image, out_image)\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "9e85eb7b",
        "execution": {
          "iopub.status.busy": "2023-05-03T18:37:16.952970Z",
          "iopub.execute_input": "2023-05-03T18:37:16.953780Z",
          "iopub.status.idle": "2023-05-03T18:37:16.963128Z",
          "shell.execute_reply.started": "2023-05-03T18:37:16.953746Z",
          "shell.execute_reply": "2023-05-03T18:37:16.962148Z"
        },
        "trusted": true
      },
      "execution_count": 11,
      "outputs": [],
      "id": "9e85eb7b"
    },
    {
      "cell_type": "code",
      "source": [
        "b = Generator()\n",
        "print(b.summary())"
      ],
      "metadata": {
        "id": "f0c76602",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9cd9ad4-9e5f-4a1f-ad70-8b27b30ccf01",
        "execution": {
          "iopub.status.busy": "2023-05-03T18:37:16.966088Z",
          "iopub.execute_input": "2023-05-03T18:37:16.966362Z",
          "iopub.status.idle": "2023-05-03T18:37:17.503375Z",
          "shell.execute_reply.started": "2023-05-03T18:37:16.966339Z",
          "shell.execute_reply": "2023-05-03T18:37:17.502570Z"
        },
        "trusted": true
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 128, 128, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 64, 64, 64)   1792        ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " leaky_re_lu_6 (LeakyReLU)      (None, 64, 64, 64)   0           ['conv2d_7[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 32, 32, 128)  73856       ['leaky_re_lu_6[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 32, 32, 128)  512        ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " leaky_re_lu_7 (LeakyReLU)      (None, 32, 32, 128)  0           ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 16, 16, 256)  295168      ['leaky_re_lu_7[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 16, 16, 256)  1024       ['conv2d_9[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " leaky_re_lu_8 (LeakyReLU)      (None, 16, 16, 256)  0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 8, 8, 512)    1180160     ['leaky_re_lu_8[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 8, 8, 512)   2048        ['conv2d_10[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " leaky_re_lu_9 (LeakyReLU)      (None, 8, 8, 512)    0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 4, 4, 512)    2359808     ['leaky_re_lu_9[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 4, 4, 512)   2048        ['conv2d_11[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " leaky_re_lu_10 (LeakyReLU)     (None, 4, 4, 512)    0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 2, 2, 512)    2359808     ['leaky_re_lu_10[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 2, 2, 512)   2048        ['conv2d_12[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " leaky_re_lu_11 (LeakyReLU)     (None, 2, 2, 512)    0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 1, 1, 512)    4194816     ['leaky_re_lu_11[0][0]']         \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 1, 1, 512)    0           ['conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " up_sampling2d (UpSampling2D)   (None, 2, 2, 512)    0           ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 2, 2, 512)    262656      ['up_sampling2d[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 2, 2, 512)   2048        ['conv2d_14[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 2, 2, 512)    0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 2, 2, 1024)   0           ['dropout[0][0]',                \n",
            "                                                                  'leaky_re_lu_11[0][0]']         \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 2, 2, 1024)   0           ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " up_sampling2d_1 (UpSampling2D)  (None, 4, 4, 1024)  0           ['activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 4, 4, 512)    524800      ['up_sampling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 4, 4, 512)   2048        ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 4, 4, 512)    0           ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 4, 4, 1024)   0           ['dropout_1[0][0]',              \n",
            "                                                                  'leaky_re_lu_10[0][0]']         \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 4, 4, 1024)   0           ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " up_sampling2d_2 (UpSampling2D)  (None, 8, 8, 1024)  0           ['activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 8, 8, 512)    524800      ['up_sampling2d_2[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 8, 8, 512)   2048        ['conv2d_16[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 8, 8, 1024)   0           ['batch_normalization_12[0][0]', \n",
            "                                                                  'leaky_re_lu_9[0][0]']          \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 8, 8, 1024)   0           ['concatenate_3[0][0]']          \n",
            "                                                                                                  \n",
            " up_sampling2d_3 (UpSampling2D)  (None, 16, 16, 1024  0          ['activation_4[0][0]']           \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 16, 16, 256)  262400      ['up_sampling2d_3[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 16, 16, 256)  1024       ['conv2d_17[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate)    (None, 16, 16, 512)  0           ['batch_normalization_13[0][0]', \n",
            "                                                                  'leaky_re_lu_8[0][0]']          \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 16, 16, 512)  0           ['concatenate_4[0][0]']          \n",
            "                                                                                                  \n",
            " up_sampling2d_4 (UpSampling2D)  (None, 32, 32, 512)  0          ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 32, 32, 128)  65664       ['up_sampling2d_4[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 32, 32, 128)  512        ['conv2d_18[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " concatenate_5 (Concatenate)    (None, 32, 32, 256)  0           ['batch_normalization_14[0][0]', \n",
            "                                                                  'leaky_re_lu_7[0][0]']          \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 32, 32, 256)  0           ['concatenate_5[0][0]']          \n",
            "                                                                                                  \n",
            " up_sampling2d_5 (UpSampling2D)  (None, 64, 64, 256)  0          ['activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 64, 64, 64)   16448       ['up_sampling2d_5[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 64, 64, 64)  256         ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " concatenate_6 (Concatenate)    (None, 64, 64, 128)  0           ['batch_normalization_15[0][0]', \n",
            "                                                                  'leaky_re_lu_6[0][0]']          \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 64, 64, 128)  0           ['concatenate_6[0][0]']          \n",
            "                                                                                                  \n",
            " up_sampling2d_6 (UpSampling2D)  (None, 128, 128, 12  0          ['activation_7[0][0]']           \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 128, 128, 64  8256        ['up_sampling2d_6[0][0]']        \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 128, 128, 3)  3075        ['conv2d_20[0][0]']              \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 128, 128, 3)  0           ['conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 12,149,123\n",
            "Trainable params: 12,141,315\n",
            "Non-trainable params: 7,808\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "id": "f0c76602"
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the discriminator model\n",
        "# tf.keras.utils.plot_model(b,show_shapes=True,\n",
        "#                           show_dtype=True,\n",
        "#                           show_layer_names=True,\n",
        "#                           show_layer_activations=True)"
      ],
      "metadata": {
        "id": "a06c11ce",
        "execution": {
          "iopub.status.busy": "2023-05-03T18:37:17.504466Z",
          "iopub.execute_input": "2023-05-03T18:37:17.504791Z",
          "iopub.status.idle": "2023-05-03T18:37:18.720114Z",
          "shell.execute_reply.started": "2023-05-03T18:37:17.504760Z",
          "shell.execute_reply": "2023-05-03T18:37:18.719054Z"
        },
        "trusted": true
      },
      "execution_count": 13,
      "outputs": [],
      "id": "a06c11ce"
    },
    {
      "cell_type": "code",
      "source": [
        "# define the combined generator and discriminator model, for updating the generator\n",
        "def GAN(g_model, d_model, image_shape = (128, 128, 3)):\n",
        "    \n",
        "    # make weights in the discriminator not trainable\n",
        "    for layer in d_model.layers:\n",
        "        if not isinstance(layer, BatchNormalization):\n",
        "            layer.trainable = False\n",
        "    \n",
        "   \n",
        "    in_src = Input(shape=image_shape)\n",
        "   \n",
        "    gen_out = g_model(in_src)\n",
        "    \n",
        "    dis_out = d_model([in_src, gen_out])\n",
        "    \n",
        "    model = Model(in_src, [dis_out, gen_out])\n",
        "    \n",
        "    # compile model\n",
        "    opt = Adam(learning_rate=0.0004, beta_1=0.5)\n",
        "    model.compile(loss=['binary_crossentropy', 'mae'], optimizer=opt, loss_weights=[1,100])\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "5c4c11b8",
        "execution": {
          "iopub.status.busy": "2023-05-03T18:37:18.721249Z",
          "iopub.execute_input": "2023-05-03T18:37:18.721931Z",
          "iopub.status.idle": "2023-05-03T18:37:18.729088Z",
          "shell.execute_reply.started": "2023-05-03T18:37:18.721899Z",
          "shell.execute_reply": "2023-05-03T18:37:18.728058Z"
        },
        "trusted": true
      },
      "execution_count": 14,
      "outputs": [],
      "id": "5c4c11b8"
    },
    {
      "cell_type": "code",
      "source": [
        "# c = GAN(b, a)\n",
        "# print(c.summary())"
      ],
      "metadata": {
        "id": "ed99a70c",
        "execution": {
          "iopub.status.busy": "2023-05-03T18:37:18.730638Z",
          "iopub.execute_input": "2023-05-03T18:37:18.731666Z",
          "iopub.status.idle": "2023-05-03T18:37:19.078988Z",
          "shell.execute_reply.started": "2023-05-03T18:37:18.731637Z",
          "shell.execute_reply": "2023-05-03T18:37:19.078060Z"
        },
        "trusted": true
      },
      "execution_count": 15,
      "outputs": [],
      "id": "ed99a70c"
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the discriminator model\n",
        "# tf.keras.utils.plot_model(c,show_shapes=True,\n",
        "#                           show_dtype=True,\n",
        "#                           show_layer_names=True,\n",
        "#                           show_layer_activations=True)"
      ],
      "metadata": {
        "id": "91897689",
        "execution": {
          "iopub.status.busy": "2023-05-03T18:37:19.080657Z",
          "iopub.execute_input": "2023-05-03T18:37:19.081078Z",
          "iopub.status.idle": "2023-05-03T18:37:19.167361Z",
          "shell.execute_reply.started": "2023-05-03T18:37:19.081025Z",
          "shell.execute_reply": "2023-05-03T18:37:19.166494Z"
        },
        "trusted": true
      },
      "execution_count": 16,
      "outputs": [],
      "id": "91897689"
    },
    {
      "cell_type": "code",
      "source": [
        "def show_prediction(bw_images, rgb_images, predicted_rgb, epoch, n, save=False):\n",
        "    \n",
        "  plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "  for i in range(n):\n",
        "    plt.suptitle(\"Iteration \"+str(epoch),size=8.5)\n",
        "      \n",
        "    plt.subplot(331)\n",
        "    plt.imshow(bw_images[i], cmap='gray')\n",
        "    plt.title(\"RGB Image\",size=8.5)\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(332)\n",
        "    plt.imshow(rgb_images[i])\n",
        "    plt.title(\"Original Segmented Image\",size=8.5)\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(333)\n",
        "    plt.imshow(predicted_rgb[i])\n",
        "    plt.title(\"Predicted Segmented Image\",size=8.5)\n",
        "    plt.axis('off')\n",
        "\n",
        "    if(save==True):\n",
        "      plt.savefig(\"/content/drive/MyDrive/Image Segmentation/Stage 2/Image_Outputs/Iteration \"+str(epoch))\n",
        "    \n",
        "    # plt.show()\n",
        "    \n",
        "# plot data\n",
        "# save_plot(k[:16])"
      ],
      "metadata": {
        "id": "f6a35054",
        "execution": {
          "iopub.status.busy": "2023-05-03T18:37:19.170314Z",
          "iopub.execute_input": "2023-05-03T18:37:19.172653Z",
          "iopub.status.idle": "2023-05-03T18:37:19.182033Z",
          "shell.execute_reply.started": "2023-05-03T18:37:19.172616Z",
          "shell.execute_reply": "2023-05-03T18:37:19.181189Z"
        },
        "trusted": true
      },
      "execution_count": 17,
      "outputs": [],
      "id": "f6a35054"
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    data = np.load(\"/content/drive/MyDrive/Image Segmentation/Datasets/street_data_128.npz\")\n",
        "    rgb, bw = data['a'], data['b']\n",
        "    rgb, bw = np.array(rgb), np.array(bw)\n",
        "    rgb, bw = rgb.astype('float32'), bw.astype('float32')\n",
        "    rgb = rgb.reshape((rgb.shape[0], rgb.shape[1], rgb.shape[2], 3))\n",
        "    bw = bw.reshape((bw.shape[0], bw.shape[1], bw.shape[2], 3))\n",
        "    # scale from [0,255] to [-1,1]\n",
        "    rgb = (rgb - 127.5) / 127.5\n",
        "    bw = (bw - 127.5) / 127.5\n",
        "    return bw, rgb\n",
        "    \n",
        "# rgb, bw = load_data()\n",
        "# print(rgb.shape, bw.shape)\n",
        "# show_prediction(rgb, bw, bw, epoch=0, n=1, save=False)"
      ],
      "metadata": {
        "id": "da97a1bd",
        "execution": {
          "iopub.status.busy": "2023-05-03T18:37:19.188621Z",
          "iopub.execute_input": "2023-05-03T18:37:19.190753Z",
          "iopub.status.idle": "2023-05-03T18:37:19.198871Z",
          "shell.execute_reply.started": "2023-05-03T18:37:19.190721Z",
          "shell.execute_reply": "2023-05-03T18:37:19.198058Z"
        },
        "trusted": true
      },
      "execution_count": 18,
      "outputs": [],
      "id": "da97a1bd"
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_real_samples(rgb_images, bw_images, n_samples, patch_size):\n",
        "    ix = randint(0, rgb_images.shape[0], n_samples)\n",
        "    rgb_images = rgb_images[ix]\n",
        "    bw_images = bw_images[ix]\n",
        "    y = ones((n_samples, patch_size, patch_size, 1))\n",
        "    return [bw_images, rgb_images], y\n",
        "\n",
        "# d = generate_real_samples(rgb, bw, 32, 8)\n",
        "# print(np.array(d[0][0]).shape, np.array(d[0][1]).shape, np.array(d[1]).shape)\n",
        "# d[1][0]"
      ],
      "metadata": {
        "id": "a883c301",
        "execution": {
          "iopub.status.busy": "2023-05-03T18:37:19.200262Z",
          "iopub.execute_input": "2023-05-03T18:37:19.200824Z",
          "iopub.status.idle": "2023-05-03T18:37:19.216122Z",
          "shell.execute_reply.started": "2023-05-03T18:37:19.200794Z",
          "shell.execute_reply": "2023-05-03T18:37:19.215157Z"
        },
        "trusted": true
      },
      "execution_count": 19,
      "outputs": [],
      "id": "a883c301"
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_fake_samples(generator, bw_images, n_samples, patch_size):\n",
        "    pred_rgb_images = generator.predict(bw_images,verbose=0)\n",
        "    y = zeros((n_samples, patch_size, patch_size, 1))\n",
        "    return pred_rgb_images, y\n",
        "\n",
        "# with tf.device(device_name):\n",
        "# kh = generate_fake_samples(b, 512, 3)\n",
        "# print(\"shape of the generated images: \",kh[0][0].shape)"
      ],
      "metadata": {
        "id": "8b981902",
        "execution": {
          "iopub.status.busy": "2023-05-03T18:37:19.217095Z",
          "iopub.execute_input": "2023-05-03T18:37:19.218180Z",
          "iopub.status.idle": "2023-05-03T18:37:19.227078Z",
          "shell.execute_reply.started": "2023-05-03T18:37:19.218150Z",
          "shell.execute_reply": "2023-05-03T18:37:19.226135Z"
        },
        "trusted": true
      },
      "execution_count": 20,
      "outputs": [],
      "id": "8b981902"
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_the_model(generator, rgb_images, bw_images, epoch, n_samples=1, save=False):\n",
        "    kk = 781\n",
        "    bw_images = bw_images[kk]\n",
        "    bw_images = bw_images.reshape((1, bw_images.shape[0], bw_images.shape[1], 3))\n",
        "    rgb_images = rgb_images[kk]\n",
        "    rgb_images = rgb_images.reshape((1, rgb_images.shape[0], rgb_images.shape[1], 3))\n",
        "    predicted_rgb  = generator.predict(bw_images,verbose=0)\n",
        "    # scale from [-1,1] to [0,1]\n",
        "    # X = (X + 1) / 2.0\n",
        "    bw_images = (bw_images + 1.0) / 2.0\n",
        "    rgb_images = (rgb_images + 1.0) / 2.0\n",
        "    predicted_rgb = (predicted_rgb + 1.0) / 2.0\n",
        "    show_prediction(bw_images, rgb_images, predicted_rgb, epoch, n=n_samples, save=save)"
      ],
      "metadata": {
        "id": "5b86702d",
        "execution": {
          "iopub.status.busy": "2023-05-03T18:37:19.228537Z",
          "iopub.execute_input": "2023-05-03T18:37:19.229466Z",
          "iopub.status.idle": "2023-05-03T18:37:19.238249Z",
          "shell.execute_reply.started": "2023-05-03T18:37:19.229436Z",
          "shell.execute_reply": "2023-05-03T18:37:19.237241Z"
        },
        "trusted": true
      },
      "execution_count": 21,
      "outputs": [],
      "id": "5b86702d"
    },
    {
      "cell_type": "code",
      "source": [
        "def train(g_model, d_model, gan_model, dataset, n_epochs=3, n_batch=128):\n",
        "    \n",
        "    rgb_images, bw_images = dataset\n",
        "    \n",
        "    print(\"Data Size: \",rgb_images.shape)\n",
        "    print(\"No. Of Epoch: \",n_epochs)\n",
        "    bat_per_epo = int(rgb_images.shape[0] / n_batch)\n",
        "    print(\"Batch Per Epoch: \", bat_per_epo)\n",
        "    print(\"Full Batch: \",n_batch)\n",
        "    print(\"*\"*50,'\\n\\n')\n",
        "    \n",
        "    patch_size = d_model.output_shape[1]\n",
        "\n",
        "    d_loss_real_array,d_loss_fake_array =[],[]\n",
        "    g_loss_array = []\n",
        "    for i in range(n_epochs):\n",
        "        d_loss_r,d_loss_f = 0.0,0.0\n",
        "        g_loss = 0.0\n",
        "\n",
        "        for j in range(bat_per_epo):\n",
        "\n",
        "            [bw_real, rgb_real], y_real = generate_real_samples(rgb_images, bw_images, n_batch, patch_size)\n",
        "            d_loss1  = d_model.train_on_batch([bw_real, rgb_real], y_real)\n",
        "            d_loss_r += (d_loss1/n_batch)\n",
        "            # print(\"real_loss\")\n",
        "\n",
        "            pred_rgb, y_fake = generate_fake_samples(g_model, bw_real, n_batch, patch_size) # Trick 1 is Here\n",
        "            d_loss2  = d_model.train_on_batch([bw_real, pred_rgb], y_fake)\n",
        "            d_loss_f += (d_loss2/n_batch)\n",
        "            # print(\"fake_loss\")\n",
        "\n",
        "            g_loss_1, _ , _ = gan_model.train_on_batch(bw_real, [y_real, rgb_real]) # Trick 2 is Here\n",
        "            g_loss += (g_loss_1/n_batch)\n",
        "            # print(\"gan_loss\")\n",
        "\n",
        "            d_loss_real_array.append(d_loss_r)\n",
        "            d_loss_fake_array.append(d_loss_f)\n",
        "            g_loss_array.append(g_loss)\n",
        "\n",
        "            # print(\"\\n\")\n",
        "\n",
        "        print('epoch -> [%d/%d], discriminator_loss_for_real_data = %f, discriminator_loss_for_fake_data = %f, generator_loss = %f\\n' %(i+1, n_epochs, d_loss_r, d_loss_f, g_loss))\n",
        "        epoch_k = i + 1\n",
        "        if(epoch_k%100==0):\n",
        "          summarize_the_model(g_model, rgb_images, bw_images, epoch_k, n_samples = 1, save=True)\n",
        "\n",
        "        d_model_json = d_model.to_json()\n",
        "        with open(\"/content/drive/MyDrive/Image Segmentation/Stage 2/PixToPixGAN_Discriminator_Model.json\", \"w\") as json_file:\n",
        "          json_file.write(d_model_json)\n",
        "\n",
        "        g_model_json = g_model.to_json()\n",
        "        with open(\"/content/drive/MyDrive/Image Segmentation/Stage 2/PixToPixGAN_Generator_Model.json\", \"w\") as json_file:\n",
        "          json_file.write(g_model_json)\n",
        "\n",
        "        # d_model.save(\"/content/drive/MyDrive/Image Segmentation/Stage 2/PixToPixGAN_Discriminator_Model.h5\")\n",
        "        # g_model.save(\"/content/drive/MyDrive/Image Segmentation/Stage 2/PixToPixGAN_Generator_Model.h5\")\n",
        "        np.savez_compressed(\"/content/drive/MyDrive/Image Segmentation/Stage 2/PixToPixGAN_Loss_Record.npz\", a=d_loss_real_array, b=d_loss_fake_array, c=g_loss_array)\n",
        "\n",
        "        # print(\"\\n\")\n",
        "\n",
        "    return d_loss_real_array, d_loss_fake_array, g_loss_array"
      ],
      "metadata": {
        "id": "a7ef034d",
        "execution": {
          "iopub.status.busy": "2023-05-03T18:37:19.239781Z",
          "iopub.execute_input": "2023-05-03T18:37:19.240510Z",
          "iopub.status.idle": "2023-05-03T18:37:19.254915Z",
          "shell.execute_reply.started": "2023-05-03T18:37:19.240481Z",
          "shell.execute_reply": "2023-05-03T18:37:19.253924Z"
        },
        "trusted": true
      },
      "execution_count": 22,
      "outputs": [],
      "id": "a7ef034d"
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device(device_name):\n",
        "\n",
        "    n_epochs = 1500\n",
        "    n_batch = 64\n",
        "    \n",
        "    d_model = Discriminator()\n",
        "    g_model = Generator()\n",
        "    gan_model = GAN(g_model, d_model)\n",
        "    \n",
        "    dataset = load_data()\n",
        "    print('\\nREADY TO GO !!!\\n')\n",
        "\n",
        "    d_loss_real_array, d_loss_fake_array, g_loss_array = train(g_model, d_model, gan_model, dataset, n_epochs, n_batch)"
      ],
      "metadata": {
        "id": "6347270c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "483085ba-1123-4b00-a631-2fd0edf19276",
        "execution": {
          "iopub.status.busy": "2023-05-03T18:37:19.256195Z",
          "iopub.execute_input": "2023-05-03T18:37:19.256689Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "READY TO GO !!!\n",
            "\n",
            "Data Size:  (2975, 128, 128, 3)\n",
            "No. Of Epoch:  1500\n",
            "Batch Per Epoch:  46\n",
            "Full Batch:  64\n",
            "************************************************** \n",
            "\n",
            "\n",
            "epoch -> [1/1500], discriminator_loss_for_real_data = 1.527608, discriminator_loss_for_fake_data = 0.998118, generator_loss = 23.480594\n",
            "\n",
            "epoch -> [2/1500], discriminator_loss_for_real_data = 1.408846, discriminator_loss_for_fake_data = 0.899095, generator_loss = 23.731913\n",
            "\n",
            "epoch -> [3/1500], discriminator_loss_for_real_data = 1.800645, discriminator_loss_for_fake_data = 1.567196, generator_loss = 19.358227\n",
            "\n",
            "epoch -> [4/1500], discriminator_loss_for_real_data = 1.443317, discriminator_loss_for_fake_data = 1.024429, generator_loss = 19.143002\n",
            "\n",
            "epoch -> [5/1500], discriminator_loss_for_real_data = 1.090233, discriminator_loss_for_fake_data = 0.989465, generator_loss = 19.481443\n",
            "\n",
            "epoch -> [6/1500], discriminator_loss_for_real_data = 0.665266, discriminator_loss_for_fake_data = 1.126401, generator_loss = 17.234163\n",
            "\n",
            "epoch -> [7/1500], discriminator_loss_for_real_data = 0.380683, discriminator_loss_for_fake_data = 0.820492, generator_loss = 19.091289\n",
            "\n",
            "epoch -> [8/1500], discriminator_loss_for_real_data = 0.376578, discriminator_loss_for_fake_data = 0.615669, generator_loss = 17.488086\n",
            "\n",
            "epoch -> [9/1500], discriminator_loss_for_real_data = 0.134359, discriminator_loss_for_fake_data = 0.284130, generator_loss = 26.160475\n",
            "\n",
            "epoch -> [10/1500], discriminator_loss_for_real_data = 0.036241, discriminator_loss_for_fake_data = 0.039526, generator_loss = 30.398498\n",
            "\n",
            "epoch -> [11/1500], discriminator_loss_for_real_data = 0.011037, discriminator_loss_for_fake_data = 0.003872, generator_loss = 27.651335\n",
            "\n",
            "epoch -> [12/1500], discriminator_loss_for_real_data = 0.003596, discriminator_loss_for_fake_data = 0.000621, generator_loss = 26.330680\n",
            "\n",
            "epoch -> [13/1500], discriminator_loss_for_real_data = 0.002155, discriminator_loss_for_fake_data = 0.001553, generator_loss = 25.785250\n",
            "\n",
            "epoch -> [14/1500], discriminator_loss_for_real_data = 0.002078, discriminator_loss_for_fake_data = 0.000470, generator_loss = 25.227562\n",
            "\n",
            "epoch -> [15/1500], discriminator_loss_for_real_data = 0.001626, discriminator_loss_for_fake_data = 0.000991, generator_loss = 24.972724\n",
            "\n",
            "epoch -> [16/1500], discriminator_loss_for_real_data = 0.001376, discriminator_loss_for_fake_data = 0.000763, generator_loss = 24.800115\n",
            "\n",
            "epoch -> [17/1500], discriminator_loss_for_real_data = 0.001316, discriminator_loss_for_fake_data = 0.000367, generator_loss = 24.139365\n",
            "\n",
            "epoch -> [18/1500], discriminator_loss_for_real_data = 0.001235, discriminator_loss_for_fake_data = 0.000237, generator_loss = 23.993500\n",
            "\n",
            "epoch -> [19/1500], discriminator_loss_for_real_data = 0.001107, discriminator_loss_for_fake_data = 0.000128, generator_loss = 23.591954\n",
            "\n",
            "epoch -> [20/1500], discriminator_loss_for_real_data = 0.000643, discriminator_loss_for_fake_data = 0.000091, generator_loss = 23.621074\n",
            "\n",
            "epoch -> [21/1500], discriminator_loss_for_real_data = 0.000657, discriminator_loss_for_fake_data = 0.000102, generator_loss = 23.404658\n",
            "\n",
            "epoch -> [22/1500], discriminator_loss_for_real_data = 0.000747, discriminator_loss_for_fake_data = 0.000229, generator_loss = 23.383588\n",
            "\n",
            "epoch -> [23/1500], discriminator_loss_for_real_data = 0.000814, discriminator_loss_for_fake_data = 0.000159, generator_loss = 23.096943\n",
            "\n",
            "epoch -> [24/1500], discriminator_loss_for_real_data = 0.121457, discriminator_loss_for_fake_data = 0.039568, generator_loss = 23.404817\n",
            "\n",
            "epoch -> [25/1500], discriminator_loss_for_real_data = 0.037893, discriminator_loss_for_fake_data = 0.023730, generator_loss = 23.693920\n",
            "\n",
            "epoch -> [26/1500], discriminator_loss_for_real_data = 0.001306, discriminator_loss_for_fake_data = 0.000050, generator_loss = 23.076135\n",
            "\n",
            "epoch -> [27/1500], discriminator_loss_for_real_data = 0.001025, discriminator_loss_for_fake_data = 0.000154, generator_loss = 22.621129\n",
            "\n",
            "epoch -> [28/1500], discriminator_loss_for_real_data = 0.000851, discriminator_loss_for_fake_data = 0.000133, generator_loss = 22.626883\n",
            "\n",
            "epoch -> [29/1500], discriminator_loss_for_real_data = 0.056209, discriminator_loss_for_fake_data = 0.130429, generator_loss = 24.443652\n",
            "\n",
            "epoch -> [30/1500], discriminator_loss_for_real_data = 0.002162, discriminator_loss_for_fake_data = 0.001134, generator_loss = 23.163608\n",
            "\n",
            "epoch -> [31/1500], discriminator_loss_for_real_data = 0.000715, discriminator_loss_for_fake_data = 0.000281, generator_loss = 22.917147\n",
            "\n",
            "epoch -> [32/1500], discriminator_loss_for_real_data = 0.000450, discriminator_loss_for_fake_data = 0.000192, generator_loss = 22.458921\n",
            "\n",
            "epoch -> [33/1500], discriminator_loss_for_real_data = 0.000391, discriminator_loss_for_fake_data = 0.000095, generator_loss = 22.479248\n",
            "\n",
            "epoch -> [34/1500], discriminator_loss_for_real_data = 0.000460, discriminator_loss_for_fake_data = 0.000118, generator_loss = 22.469947\n",
            "\n",
            "epoch -> [35/1500], discriminator_loss_for_real_data = 0.000790, discriminator_loss_for_fake_data = 0.000037, generator_loss = 22.184536\n",
            "\n",
            "epoch -> [36/1500], discriminator_loss_for_real_data = 0.000290, discriminator_loss_for_fake_data = 0.000136, generator_loss = 22.126356\n",
            "\n",
            "epoch -> [37/1500], discriminator_loss_for_real_data = 0.000178, discriminator_loss_for_fake_data = 0.000034, generator_loss = 22.077019\n",
            "\n",
            "epoch -> [38/1500], discriminator_loss_for_real_data = 0.422775, discriminator_loss_for_fake_data = 0.536056, generator_loss = 20.910579\n",
            "\n",
            "epoch -> [39/1500], discriminator_loss_for_real_data = 0.044706, discriminator_loss_for_fake_data = 0.073729, generator_loss = 22.356059\n",
            "\n",
            "epoch -> [40/1500], discriminator_loss_for_real_data = 0.000633, discriminator_loss_for_fake_data = 0.000343, generator_loss = 22.209503\n",
            "\n",
            "epoch -> [41/1500], discriminator_loss_for_real_data = 0.000398, discriminator_loss_for_fake_data = 0.000215, generator_loss = 21.983601\n",
            "\n",
            "epoch -> [42/1500], discriminator_loss_for_real_data = 0.000291, discriminator_loss_for_fake_data = 0.000102, generator_loss = 21.862636\n",
            "\n",
            "epoch -> [43/1500], discriminator_loss_for_real_data = 0.000212, discriminator_loss_for_fake_data = 0.000065, generator_loss = 21.640570\n",
            "\n",
            "epoch -> [44/1500], discriminator_loss_for_real_data = 0.000167, discriminator_loss_for_fake_data = 0.000066, generator_loss = 21.699646\n",
            "\n",
            "epoch -> [45/1500], discriminator_loss_for_real_data = 0.000156, discriminator_loss_for_fake_data = 0.000174, generator_loss = 21.457567\n",
            "\n",
            "epoch -> [46/1500], discriminator_loss_for_real_data = 0.000111, discriminator_loss_for_fake_data = 0.000027, generator_loss = 21.578130\n",
            "\n",
            "epoch -> [47/1500], discriminator_loss_for_real_data = 0.000099, discriminator_loss_for_fake_data = 0.000032, generator_loss = 21.459179\n",
            "\n",
            "epoch -> [48/1500], discriminator_loss_for_real_data = 0.000103, discriminator_loss_for_fake_data = 0.000026, generator_loss = 21.340144\n",
            "\n",
            "epoch -> [49/1500], discriminator_loss_for_real_data = 0.000088, discriminator_loss_for_fake_data = 0.000035, generator_loss = 21.213519\n",
            "\n",
            "epoch -> [50/1500], discriminator_loss_for_real_data = 0.000080, discriminator_loss_for_fake_data = 0.000019, generator_loss = 21.232714\n",
            "\n",
            "epoch -> [51/1500], discriminator_loss_for_real_data = 0.000068, discriminator_loss_for_fake_data = 0.000027, generator_loss = 21.167111\n",
            "\n",
            "epoch -> [52/1500], discriminator_loss_for_real_data = 0.000066, discriminator_loss_for_fake_data = 0.000043, generator_loss = 21.137500\n",
            "\n",
            "epoch -> [53/1500], discriminator_loss_for_real_data = 0.000064, discriminator_loss_for_fake_data = 0.000014, generator_loss = 20.972695\n",
            "\n",
            "epoch -> [54/1500], discriminator_loss_for_real_data = 0.000064, discriminator_loss_for_fake_data = 0.000046, generator_loss = 20.962077\n",
            "\n",
            "epoch -> [55/1500], discriminator_loss_for_real_data = 0.000049, discriminator_loss_for_fake_data = 0.000002, generator_loss = 20.918930\n",
            "\n",
            "epoch -> [56/1500], discriminator_loss_for_real_data = 0.000049, discriminator_loss_for_fake_data = 0.000020, generator_loss = 20.856467\n",
            "\n",
            "epoch -> [57/1500], discriminator_loss_for_real_data = 0.000046, discriminator_loss_for_fake_data = 0.000024, generator_loss = 20.665224\n",
            "\n",
            "epoch -> [58/1500], discriminator_loss_for_real_data = 0.000044, discriminator_loss_for_fake_data = 0.000013, generator_loss = 20.638652\n",
            "\n",
            "epoch -> [59/1500], discriminator_loss_for_real_data = 0.000046, discriminator_loss_for_fake_data = 0.000083, generator_loss = 20.614671\n",
            "\n",
            "epoch -> [60/1500], discriminator_loss_for_real_data = 0.000067, discriminator_loss_for_fake_data = 0.000013, generator_loss = 20.571584\n",
            "\n",
            "epoch -> [61/1500], discriminator_loss_for_real_data = 0.508904, discriminator_loss_for_fake_data = 0.340445, generator_loss = 18.802998\n",
            "\n",
            "epoch -> [62/1500], discriminator_loss_for_real_data = 0.424188, discriminator_loss_for_fake_data = 0.489692, generator_loss = 24.123470\n",
            "\n",
            "epoch -> [63/1500], discriminator_loss_for_real_data = 0.003176, discriminator_loss_for_fake_data = 0.004301, generator_loss = 23.284408\n",
            "\n",
            "epoch -> [64/1500], discriminator_loss_for_real_data = 0.000618, discriminator_loss_for_fake_data = 0.000609, generator_loss = 22.392458\n",
            "\n",
            "epoch -> [65/1500], discriminator_loss_for_real_data = 0.000407, discriminator_loss_for_fake_data = 0.003914, generator_loss = 22.089270\n",
            "\n",
            "epoch -> [66/1500], discriminator_loss_for_real_data = 0.000192, discriminator_loss_for_fake_data = 0.000062, generator_loss = 21.734263\n",
            "\n",
            "epoch -> [67/1500], discriminator_loss_for_real_data = 0.000164, discriminator_loss_for_fake_data = 0.000017, generator_loss = 21.561319\n",
            "\n",
            "epoch -> [68/1500], discriminator_loss_for_real_data = 0.000044, discriminator_loss_for_fake_data = 0.000030, generator_loss = 21.116190\n",
            "\n",
            "epoch -> [69/1500], discriminator_loss_for_real_data = 0.000050, discriminator_loss_for_fake_data = 0.000105, generator_loss = 20.894737\n",
            "\n",
            "epoch -> [70/1500], discriminator_loss_for_real_data = 0.001895, discriminator_loss_for_fake_data = 0.001681, generator_loss = 20.867314\n",
            "\n",
            "epoch -> [71/1500], discriminator_loss_for_real_data = 0.000111, discriminator_loss_for_fake_data = 0.000288, generator_loss = 20.735703\n",
            "\n",
            "epoch -> [72/1500], discriminator_loss_for_real_data = 0.000046, discriminator_loss_for_fake_data = 0.000002, generator_loss = 20.495000\n",
            "\n",
            "epoch -> [73/1500], discriminator_loss_for_real_data = 0.000039, discriminator_loss_for_fake_data = 0.000019, generator_loss = 20.348386\n",
            "\n",
            "epoch -> [74/1500], discriminator_loss_for_real_data = 0.000027, discriminator_loss_for_fake_data = 0.000001, generator_loss = 20.270016\n",
            "\n",
            "epoch -> [75/1500], discriminator_loss_for_real_data = 0.000021, discriminator_loss_for_fake_data = 0.000021, generator_loss = 20.236898\n",
            "\n",
            "epoch -> [76/1500], discriminator_loss_for_real_data = 0.000023, discriminator_loss_for_fake_data = 0.000047, generator_loss = 20.062077\n",
            "\n",
            "epoch -> [77/1500], discriminator_loss_for_real_data = 0.000303, discriminator_loss_for_fake_data = 0.000267, generator_loss = 19.999233\n",
            "\n",
            "epoch -> [78/1500], discriminator_loss_for_real_data = 0.000052, discriminator_loss_for_fake_data = 0.000694, generator_loss = 19.981745\n",
            "\n",
            "epoch -> [79/1500], discriminator_loss_for_real_data = 0.000021, discriminator_loss_for_fake_data = 0.000026, generator_loss = 19.939961\n",
            "\n",
            "epoch -> [80/1500], discriminator_loss_for_real_data = 0.000062, discriminator_loss_for_fake_data = 0.000352, generator_loss = 20.032629\n",
            "\n",
            "epoch -> [81/1500], discriminator_loss_for_real_data = 0.000032, discriminator_loss_for_fake_data = 0.000018, generator_loss = 19.859547\n",
            "\n",
            "epoch -> [82/1500], discriminator_loss_for_real_data = 0.000018, discriminator_loss_for_fake_data = 0.000016, generator_loss = 19.831423\n",
            "\n",
            "epoch -> [83/1500], discriminator_loss_for_real_data = 0.000018, discriminator_loss_for_fake_data = 0.000004, generator_loss = 19.811279\n",
            "\n",
            "epoch -> [84/1500], discriminator_loss_for_real_data = 0.000012, discriminator_loss_for_fake_data = 0.000002, generator_loss = 19.507803\n",
            "\n",
            "epoch -> [85/1500], discriminator_loss_for_real_data = 0.000018, discriminator_loss_for_fake_data = 0.000009, generator_loss = 19.552715\n",
            "\n",
            "epoch -> [86/1500], discriminator_loss_for_real_data = 0.000011, discriminator_loss_for_fake_data = 0.000001, generator_loss = 19.556820\n",
            "\n",
            "epoch -> [87/1500], discriminator_loss_for_real_data = 0.000010, discriminator_loss_for_fake_data = 0.000000, generator_loss = 19.570604\n",
            "\n",
            "epoch -> [88/1500], discriminator_loss_for_real_data = 0.000009, discriminator_loss_for_fake_data = 0.000002, generator_loss = 19.472721\n",
            "\n",
            "epoch -> [89/1500], discriminator_loss_for_real_data = 0.000007, discriminator_loss_for_fake_data = 0.000000, generator_loss = 19.317517\n",
            "\n",
            "epoch -> [90/1500], discriminator_loss_for_real_data = 0.000007, discriminator_loss_for_fake_data = 0.000000, generator_loss = 19.242310\n",
            "\n",
            "epoch -> [91/1500], discriminator_loss_for_real_data = 0.000007, discriminator_loss_for_fake_data = 0.000006, generator_loss = 19.272368\n",
            "\n",
            "epoch -> [92/1500], discriminator_loss_for_real_data = 0.000009, discriminator_loss_for_fake_data = 0.000001, generator_loss = 19.174548\n",
            "\n",
            "epoch -> [93/1500], discriminator_loss_for_real_data = 0.000008, discriminator_loss_for_fake_data = 0.000000, generator_loss = 19.185442\n",
            "\n",
            "epoch -> [94/1500], discriminator_loss_for_real_data = 0.000005, discriminator_loss_for_fake_data = 0.000000, generator_loss = 19.162129\n",
            "\n",
            "epoch -> [95/1500], discriminator_loss_for_real_data = 0.000007, discriminator_loss_for_fake_data = 0.000011, generator_loss = 19.144172\n",
            "\n",
            "epoch -> [96/1500], discriminator_loss_for_real_data = 0.000005, discriminator_loss_for_fake_data = 0.000000, generator_loss = 19.069310\n",
            "\n",
            "epoch -> [97/1500], discriminator_loss_for_real_data = 0.000007, discriminator_loss_for_fake_data = 0.000026, generator_loss = 19.179581\n",
            "\n",
            "epoch -> [98/1500], discriminator_loss_for_real_data = 0.000008, discriminator_loss_for_fake_data = 0.000002, generator_loss = 19.002115\n",
            "\n",
            "epoch -> [99/1500], discriminator_loss_for_real_data = 0.000005, discriminator_loss_for_fake_data = 0.000000, generator_loss = 18.932562\n",
            "\n",
            "epoch -> [100/1500], discriminator_loss_for_real_data = 0.000007, discriminator_loss_for_fake_data = 0.000005, generator_loss = 18.803752\n",
            "\n",
            "epoch -> [101/1500], discriminator_loss_for_real_data = 0.000010, discriminator_loss_for_fake_data = 0.000001, generator_loss = 18.792530\n",
            "\n",
            "epoch -> [102/1500], discriminator_loss_for_real_data = 0.000009, discriminator_loss_for_fake_data = 0.000015, generator_loss = 18.788186\n",
            "\n",
            "epoch -> [103/1500], discriminator_loss_for_real_data = 0.000010, discriminator_loss_for_fake_data = 0.000005, generator_loss = 18.795591\n",
            "\n",
            "epoch -> [104/1500], discriminator_loss_for_real_data = 0.000493, discriminator_loss_for_fake_data = 0.000017, generator_loss = 18.742310\n",
            "\n",
            "epoch -> [105/1500], discriminator_loss_for_real_data = 0.356224, discriminator_loss_for_fake_data = 0.069100, generator_loss = 18.870477\n",
            "\n",
            "epoch -> [106/1500], discriminator_loss_for_real_data = 3.816829, discriminator_loss_for_fake_data = 0.333961, generator_loss = 15.576492\n",
            "\n",
            "epoch -> [107/1500], discriminator_loss_for_real_data = 0.748200, discriminator_loss_for_fake_data = 0.405133, generator_loss = 13.707505\n",
            "\n",
            "epoch -> [108/1500], discriminator_loss_for_real_data = 0.522051, discriminator_loss_for_fake_data = 0.719062, generator_loss = 11.315283\n",
            "\n",
            "epoch -> [109/1500], discriminator_loss_for_real_data = 0.807369, discriminator_loss_for_fake_data = 1.023509, generator_loss = 12.649143\n",
            "\n",
            "epoch -> [110/1500], discriminator_loss_for_real_data = 0.783193, discriminator_loss_for_fake_data = 1.141438, generator_loss = 12.441710\n",
            "\n",
            "epoch -> [111/1500], discriminator_loss_for_real_data = 0.756885, discriminator_loss_for_fake_data = 1.522917, generator_loss = 11.174013\n",
            "\n",
            "epoch -> [112/1500], discriminator_loss_for_real_data = 0.531044, discriminator_loss_for_fake_data = 1.275255, generator_loss = 10.860911\n",
            "\n",
            "epoch -> [113/1500], discriminator_loss_for_real_data = 1.344767, discriminator_loss_for_fake_data = 1.494057, generator_loss = 14.620650\n",
            "\n",
            "epoch -> [114/1500], discriminator_loss_for_real_data = 2.124192, discriminator_loss_for_fake_data = 1.394729, generator_loss = 15.850002\n",
            "\n",
            "epoch -> [115/1500], discriminator_loss_for_real_data = 1.981066, discriminator_loss_for_fake_data = 0.472494, generator_loss = 14.911866\n",
            "\n",
            "epoch -> [116/1500], discriminator_loss_for_real_data = 2.001300, discriminator_loss_for_fake_data = 0.442553, generator_loss = 13.349078\n",
            "\n",
            "epoch -> [117/1500], discriminator_loss_for_real_data = 1.948819, discriminator_loss_for_fake_data = 0.401610, generator_loss = 12.721140\n",
            "\n",
            "epoch -> [118/1500], discriminator_loss_for_real_data = 2.026262, discriminator_loss_for_fake_data = 0.328206, generator_loss = 12.606518\n",
            "\n",
            "epoch -> [119/1500], discriminator_loss_for_real_data = 2.087652, discriminator_loss_for_fake_data = 0.331109, generator_loss = 13.201307\n",
            "\n",
            "epoch -> [120/1500], discriminator_loss_for_real_data = 2.075397, discriminator_loss_for_fake_data = 0.365919, generator_loss = 17.513523\n",
            "\n",
            "epoch -> [121/1500], discriminator_loss_for_real_data = 2.011315, discriminator_loss_for_fake_data = 0.339158, generator_loss = 25.108446\n",
            "\n",
            "epoch -> [122/1500], discriminator_loss_for_real_data = 2.006231, discriminator_loss_for_fake_data = 0.146097, generator_loss = 28.412868\n",
            "\n",
            "epoch -> [123/1500], discriminator_loss_for_real_data = 1.998933, discriminator_loss_for_fake_data = 0.199730, generator_loss = 26.321000\n",
            "\n",
            "epoch -> [124/1500], discriminator_loss_for_real_data = 1.981776, discriminator_loss_for_fake_data = 0.132098, generator_loss = 25.906057\n",
            "\n",
            "epoch -> [125/1500], discriminator_loss_for_real_data = 2.050207, discriminator_loss_for_fake_data = 0.008133, generator_loss = 25.332741\n",
            "\n",
            "epoch -> [126/1500], discriminator_loss_for_real_data = 2.118878, discriminator_loss_for_fake_data = 0.002707, generator_loss = 24.671934\n",
            "\n",
            "epoch -> [127/1500], discriminator_loss_for_real_data = 2.084306, discriminator_loss_for_fake_data = 0.018874, generator_loss = 25.619733\n",
            "\n",
            "epoch -> [128/1500], discriminator_loss_for_real_data = 1.957126, discriminator_loss_for_fake_data = 0.108887, generator_loss = 26.459128\n",
            "\n",
            "epoch -> [129/1500], discriminator_loss_for_real_data = 2.171538, discriminator_loss_for_fake_data = 0.059190, generator_loss = 23.653683\n",
            "\n",
            "epoch -> [130/1500], discriminator_loss_for_real_data = 2.121090, discriminator_loss_for_fake_data = 0.004316, generator_loss = 24.139542\n",
            "\n",
            "epoch -> [131/1500], discriminator_loss_for_real_data = 2.105915, discriminator_loss_for_fake_data = 0.006277, generator_loss = 25.038754\n",
            "\n",
            "epoch -> [132/1500], discriminator_loss_for_real_data = 2.017230, discriminator_loss_for_fake_data = 0.003393, generator_loss = 29.277480\n",
            "\n",
            "epoch -> [133/1500], discriminator_loss_for_real_data = 2.043674, discriminator_loss_for_fake_data = 0.000573, generator_loss = 24.866409\n",
            "\n",
            "epoch -> [134/1500], discriminator_loss_for_real_data = 1.925599, discriminator_loss_for_fake_data = 0.069379, generator_loss = 23.200992\n",
            "\n",
            "epoch -> [135/1500], discriminator_loss_for_real_data = 1.966631, discriminator_loss_for_fake_data = 0.199601, generator_loss = 21.361094\n",
            "\n",
            "epoch -> [136/1500], discriminator_loss_for_real_data = 1.295053, discriminator_loss_for_fake_data = 0.196394, generator_loss = 21.984069\n",
            "\n",
            "epoch -> [137/1500], discriminator_loss_for_real_data = 0.869964, discriminator_loss_for_fake_data = 0.015475, generator_loss = 22.686641\n",
            "\n",
            "epoch -> [138/1500], discriminator_loss_for_real_data = 0.887973, discriminator_loss_for_fake_data = 0.043089, generator_loss = 22.549379\n",
            "\n",
            "epoch -> [139/1500], discriminator_loss_for_real_data = 0.633083, discriminator_loss_for_fake_data = 0.046332, generator_loss = 22.398751\n",
            "\n",
            "epoch -> [140/1500], discriminator_loss_for_real_data = 0.540629, discriminator_loss_for_fake_data = 0.014680, generator_loss = 22.684732\n",
            "\n",
            "epoch -> [141/1500], discriminator_loss_for_real_data = 0.429653, discriminator_loss_for_fake_data = 0.024693, generator_loss = 22.552740\n",
            "\n",
            "epoch -> [142/1500], discriminator_loss_for_real_data = 0.339929, discriminator_loss_for_fake_data = 0.001888, generator_loss = 22.252942\n",
            "\n",
            "epoch -> [143/1500], discriminator_loss_for_real_data = 0.276352, discriminator_loss_for_fake_data = 0.010494, generator_loss = 22.184166\n",
            "\n",
            "epoch -> [144/1500], discriminator_loss_for_real_data = 0.239556, discriminator_loss_for_fake_data = 0.007144, generator_loss = 22.454961\n",
            "\n",
            "epoch -> [145/1500], discriminator_loss_for_real_data = 0.178956, discriminator_loss_for_fake_data = 0.004997, generator_loss = 22.109320\n",
            "\n",
            "epoch -> [146/1500], discriminator_loss_for_real_data = 0.114108, discriminator_loss_for_fake_data = 0.001468, generator_loss = 21.919239\n",
            "\n",
            "epoch -> [147/1500], discriminator_loss_for_real_data = 0.072044, discriminator_loss_for_fake_data = 0.004506, generator_loss = 21.748692\n",
            "\n",
            "epoch -> [148/1500], discriminator_loss_for_real_data = 0.045467, discriminator_loss_for_fake_data = 0.000621, generator_loss = 21.565597\n",
            "\n",
            "epoch -> [149/1500], discriminator_loss_for_real_data = 0.007547, discriminator_loss_for_fake_data = 0.000611, generator_loss = 21.511103\n",
            "\n",
            "epoch -> [150/1500], discriminator_loss_for_real_data = 0.004068, discriminator_loss_for_fake_data = 0.000415, generator_loss = 21.400884\n",
            "\n",
            "epoch -> [151/1500], discriminator_loss_for_real_data = 0.002609, discriminator_loss_for_fake_data = 0.000194, generator_loss = 21.264759\n",
            "\n",
            "epoch -> [152/1500], discriminator_loss_for_real_data = 0.002327, discriminator_loss_for_fake_data = 0.000196, generator_loss = 21.128641\n",
            "\n"
          ]
        }
      ],
      "id": "6347270c"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "00b1821b"
      },
      "execution_count": null,
      "outputs": [],
      "id": "00b1821b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Misc."
      ],
      "metadata": {
        "id": "4cbde780"
      },
      "id": "4cbde780"
    }
  ]
}